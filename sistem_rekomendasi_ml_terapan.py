# -*- coding: utf-8 -*-
"""Sistem Rekomendasi_ML Terapan

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bUvn6hGqMaXoBPgn9ua2YPKv2teECLu_

#Submission 2 - Sistem Rekomendasi

Nama : Ainun Annisa K

Dataset : [Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data)

Import Library
"""

from google.colab import files
import zipfile
import io
import os

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, precision_score, recall_score
from math import sqrt

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras import layers, regularizers

from google.colab import files
uploaded = files.upload()

zip_file_name = next(iter(uploaded))

with zipfile.ZipFile(io.BytesIO(uploaded[zip_file_name]), 'r') as zip_ref:
    zip_ref.extractall("/content")

books = pd.read_csv('/content/Dataset/Books.csv')
ratings = pd.read_csv('/content/Dataset/Ratings.csv')
users = pd.read_csv('/content/Dataset/Users.csv')

"""### Data Understanding"""

books.head()

print("Number of Book ISBN numbers:", len(books['ISBN'].unique()))
print("Number of book titles:", len(books['Book-Title'].unique()))
print('Number of book authors:', len(books['Book-Author'].unique()))
print('Number of Publication Years:', len(books['Year-Of-Publication'].unique()))
print('Number of publisher names:', len(books['Publisher'].unique()))

"""Berdasarkan dataframe books, jumlah judul buku dalam dataset adalah 242.135, sedangkan jumlah ISBN buku adalah 271.357. Hal ini menunjukkan adanya beberapa buku yang tidak memiliki nomor ISBN karena seharusnya setiap buku memiliki ISBN yang unik. Dalam hal ini, dataset akan difilter untuk memastikan setiap buku memiliki ISBN yang unik."""

ratings.head()

users.head()

"""Penerbit dengan buku terbanyak"""

top_publishers = books['Publisher'].value_counts().head(10)

plt.figure(figsize=(10, 6))
top_publishers.plot(kind='bar')
plt.title('Penerbit dengan buku terbanyak')
plt.xlabel('Penerbit')
plt.ylabel('Jumlah buku')
plt.xticks(rotation=45)
plt.show()

"""Berdasarkan informasi di atas, diketahui bahwa penerbit Harlequin merilis buku paling banyak, dengan total lebih dari 7000 buku.

Penulis dengan buku terbanyak
"""

top_authors = books['Book-Author'].value_counts().head(10)

plt.figure(figsize=(10, 6))
top_authors.plot(kind='bar')
plt.title('Penulis dengan buku terbanyak')
plt.xlabel('Penulis')
plt.ylabel('Jumlah buku')
plt.xticks(rotation=45)
plt.show()

"""Berdasarkan informasi di atas, diketahui bahwa penulis dengan nama Agatha Christie menulis buku paling banyak, dengan total lebih dari 600 buku. Dari informasi ini, juga terlihat bahwa dataset ini mencakup beberapa penulis yang telah menulis lebih dari satu judul buku.

Penulis dengan rata-rata rating tertinggi
"""

avg_rating_per_author = ratings.merge(books[['ISBN', 'Book-Author']], on='ISBN').groupby('Book-Author')['Book-Rating'].mean()
plt.figure(figsize=(10, 6))
avg_rating_per_author[:10].plot(kind='bar')
plt.title('Penulis dengan rata-rata rating tertinggi')
plt.xlabel('Penulis')
plt.ylabel('Rata-rata rating')
plt.xticks(rotation=90 )
plt.show()

"""Dari plot diatas bisa dilihat bahwa terdapat perbedaan yang cukup besar dari rata-rata rating penulis.

Kota dan negara dengan user terbanyak
"""

my_dict=(users['Location'].value_counts()).to_dict()
count= pd.DataFrame(list(my_dict.items()),columns = ['c','count'])
f = count.sort_values(by=['count'], ascending = False)
f = f.head(15)
f.drop(7,inplace=True)
fig=plt.figure(figsize=(10,5))
ax = sns.barplot(y = 'count',x= 'c' , data = f)
ax.set_xticklabels(ax.get_xticklabels(), rotation=90,horizontalalignment='center')
for bar in ax.patches:
    ax.annotate(format(bar.get_height(), '.0f'),
                   (bar.get_x() + bar.get_width() / 2,
                    bar.get_height()), ha='center', va='center',
                   size=8, xytext=(0,8),
                   textcoords='offset points')
plt.xlabel("Kota dan Negara", size=14)
plt.ylabel("Jumlah user", size=14)
plt.title("Kota dan Negara dengan user terbanyak", size=18)
plt.show()

"""Dapat dilihat pada plot diatas bahwa negara dengan user terbanyak adalah pada kota London di United Kingdom dan diikuti oleh kota Toronto di Canada.

## Data Preparation
"""

# Penghapusan 'Image-URL-S', 'Image-URL-M', 'Image-URL-L'

books.drop(labels=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1, inplace=True)

books.head()

# Menggabungkan dataframe books dan ratings berdasarkan ISBN

books = pd.merge(ratings, books, on='ISBN', how='left')
books.head()

#Cek missing values
books.isnull().sum()

# Menghapus missing values
books=books.dropna()

# Mengurutkan data berdasarkan ISBN
books= books.sort_values('ISBN', ascending=True)
books

books.info()

#Cek duplicated ISBN
books.duplicated('ISBN').sum()

# Menghapus data duplikat ISBN
books_new= books.drop_duplicates('ISBN')
books_new

"""## Modeling and Result

## Content Based Filtering
"""

books_cb = books_new[:1000]

books_cb

vectorizer = CountVectorizer()
author_vector = vectorizer.fit_transform(books_cb['Book-Author'])

# Menghitung cosine similarity antar buku berdasarkan "Book-Author"
cosine_sim_author = cosine_similarity(author_vector)

def content_based_recommendation(book_title, similarity_matrix, items_df, top_k=5):
    index = np.where(items_df['Book-Title'] == book_title)[0]

    if index:
        index = index[0]

        # Hitung similarity dengan buku lain
        similar_books = sorted(list(enumerate(similarity_matrix[index])), key=lambda x: x[1], reverse=True)[1:(top_k + 1)]
        similar_books_indices = [i[0] for i in similar_books]
        recommended_books = items_df.iloc[similar_books_indices][['Book-Title', 'Book-Author']].drop_duplicates('Book-Title')

        return recommended_books.values.tolist()

    else:
        print(f"Buku dengan judul '{book_title}' tidak ditemukan dalam dataframe.")
        return None

book_title_to_recommend = "Paddington in the Kitchen"
recommendations = content_based_recommendation(book_title_to_recommend, cosine_sim_author, books_new)

if recommendations:
    df_recommendations = pd.DataFrame(recommendations, columns=['Recommended Book', 'Author'])
    print(f"Rekomendasi untuk buku '{book_title_to_recommend}':")
    print(df_recommendations)
else:
    print(f"Tidak ada rekomendasi untuk buku '{book_title_to_recommend}'.")

df_recommendations

"""## Collaborative Filtering"""

# Convert kolom data menjadi list
isbn_id = books_new['ISBN'].tolist()
book_title = books_new['Book-Title'].tolist()
book_author = books_new['Book-Author'].tolist()
year_of_publication = books_new['Year-Of-Publication'].tolist()
publisher = books_new['Publisher'].tolist()

print(len(isbn_id))
print(len(book_title))
print(len(book_author))
print(len(year_of_publication))
print(len(publisher))

books_cf = pd.DataFrame({
    'isbn': isbn_id,
    'book_title': book_title,
    'book_author': book_author,
    'year_of_publication': year_of_publication,
    'publisher': publisher

})

books_cf

books_cf = books_cf[:20000]
books_cf

x=books.groupby('User-ID').count()['Book-Rating']>200
x.sum()

users_200=x[x].index
users_200

filtered_rating=books[books['User-ID'].isin(users_200)]
filtered_rating

y=filtered_rating.groupby('Book-Title').count()['Book-Rating']>=50
famous_books=y[y].index

final_rating=filtered_rating[filtered_rating['Book-Title'].isin(famous_books)]
final_rating

final_rating.info()

# Mengubah User-ID menjadi list
user_ids = final_rating['User-ID'].unique().tolist()
print('list userIDs: ', user_ids)

# Mengubah User-ID encoding
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID: ', user_to_user_encoded)

user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded number to userID: ', user_encoded_to_user)

# Mengubah ISBN menjadi list
isbn_id = final_rating['ISBN'].unique().tolist()

# Mengubah ISBN encoding
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_id)}
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_id)}

final_rating['user'] = final_rating['User-ID'].map(user_to_user_encoded)
final_rating['book_title'] = final_rating['ISBN'].map(isbn_to_isbn_encoded)

# Menghitung jumlah pengguna dan buku
num_users = len(user_to_user_encoded)
print(num_users)

num_book_title = len(isbn_to_isbn_encoded)
print(num_book_title)

final_rating['Book-Rating'] = final_rating['Book-Rating'].values.astype(np.float32)

# Menentukan nilai minimum dan maximum rating buku
min_rating = min(final_rating['Book-Rating'])
max_rating = max(final_rating['Book-Rating'])

print('Number of Users: {}, Number of Books: {}, Min Rating: {}, Max Rating: {}'.format(
     num_users, num_book_title, min_rating, max_rating
))

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_book_titles, embedding_size, dropout_rate=0.2, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_book_titles = num_book_titles
        self.embedding_size = embedding_size
        self.dropout_rate = dropout_rate

        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.book_title_embedding = layers.Embedding(
            num_book_titles,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=regularizers.l2(1e-6)
        )
        self.book_title_bias = layers.Embedding(num_book_titles, 1)

        self.dropout = layers.Dropout(rate=dropout_rate)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_vector = self.dropout(user_vector)
        user_bias = self.user_bias(inputs[:, 0])

        book_title_vector = self.book_title_embedding(inputs[:, 1])
        book_title_vector = self.dropout(book_title_vector)
        book_title_bias = self.book_title_bias(inputs[:, 1])

        dot_user_book_title = tf.tensordot(user_vector, book_title_vector, 2)

        x = dot_user_book_title + user_bias + book_title_bias

        return tf.nn.sigmoid(x)

final_rating = final_rating.sample(frac=1, random_state=42)
final_rating

x = final_rating[['user', 'book_title']].values
y = final_rating['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Split dataset 80% training and 20% test
train_indices = int(0.8 * final_rating.shape[0])
x_train, x_val, y_train, y_val = (
     x[:train_indices],
     x[train_indices:],
     y[:train_indices],
     y[train_indices:]
)

print(x, y)

model = RecommenderNet(num_users, num_book_title, 50)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.RootMeanSquaredError()])

history = model.fit(x = x_train, y = y_train, batch_size = 64, epochs = 100, validation_data = (x_val, y_val))

train_rmse = history.history['root_mean_squared_error']
val_rmse = history.history['val_root_mean_squared_error']

# Plot RMSE
plt.plot(train_rmse, label='Training RMSE')
plt.plot(val_rmse, label='Validation RMSE')
plt.title('Training and Validation RMSE')
plt.xlabel('Epochs')
plt.ylabel('RMSE')
plt.legend()
plt.show()

book_df = books_cf

# Mengambil sampel user
# user_id = final_rating['User-ID'].sample(1).iloc[0] # Mengambil sampel User ID secara acak
user_id = int(input("Masukkan User-ID: "))
book_readed_by_user = final_rating[final_rating['User-ID'] == user_id]

# Membuat variabel book_not_readed
book_not_readed = book_df[~book_df['isbn'].isin(book_readed_by_user['ISBN'].values)]['isbn']
book_not_readed = list(
    set(book_not_readed)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_readed = [[isbn_to_isbn_encoded.get(x)] for x in book_not_readed]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_readed), book_not_readed)
)

ratings_model = model.predict(user_book_array).flatten()

top_ratings_indices = ratings_model.argsort()[-10:][::-1]

recommended_book_ids = [
    isbn_encoded_to_isbn.get(book_not_readed[x][0]) for x in top_ratings_indices
]


top_book_user = (
    book_readed_by_user.sort_values(
        by='Book-Rating',
        ascending=False
    )
    .head(10)['ISBN'].values
)

print(top_book_user)

book_df_rows = book_df[book_df['isbn'].isin(top_book_user)]

# Menampilkan hasil rekomendasi buku dalam DataFrame
book_df_rows_data = []
for row in book_df_rows.itertuples():
    book_df_rows_data.append([row.book_title, row.book_author])

recommended_book = book_df[book_df['isbn'].isin(recommended_book_ids)]

recommended_book_data = []
for row in recommended_book.itertuples():
    recommended_book_data.append([row.book_title, row.book_author])

output_columns = ['Book Title', 'Book Author']
df_book_readed_by_user = pd.DataFrame(book_df_rows_data, columns=output_columns)
df_recommended_books = pd.DataFrame(recommended_book_data, columns=output_columns)

# Menampilkan hasil rekomendasi
print("Rekomendasi untuk User ID: {}".format(user_id))
print("===" * 9)
print("Buku dengan rating tertinggi yang diberikan oleh User ID{}".format(user_id))
print("----" * 8)
print(df_book_readed_by_user)
print("----" * 8)
print("10 Rekomendasi buku")
print("----" * 8)
df_recommended_books

